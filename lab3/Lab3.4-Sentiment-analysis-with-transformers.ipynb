{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3.4 Sentiment Classification using transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explains how you can use a transformer model that is fine-tuned for sentiment analysis. Fine-tuned transformer models are published regularly on the huggingface platform: https://huggingface.co/models\n",
    "\n",
    "These models are very big (Gigabytes) and require a computer with sufficient memory to load. Furthermore, loading these models takes some time as well. It is also possible to copy such a model to your disk and to load the local copy. Still a substantial memory is needed to load it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires installing some deep learning packages: transformers, pytorch and simpletransformers. If you are not experienced with installing these packages, make sure you first define a virtual environment for python, activate this environment and install the packages in this enviroment.\n",
    "\n",
    "Please consult the Python documentation for installing such an enviroment:\n",
    "\n",
    "https://docs.python.org/3/library/venv.html\n",
    "\n",
    "After activating your enviroment you can install pytorch, transformers and simpletransformers from the command line. If you start this notebook within the same virtual environment you can also execute the next installation commands from your notebook. Once installed, you can comment out the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install pytorch cpuonly -c pytorch\n",
    "# !pip install transformers\n",
    "# !pip install simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huggingface transfomers provides an option to create a **pipeline** to perform a NLP task with a pretrained model: \n",
    "\n",
    "\"The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering.\"\n",
    "\n",
    "More information can be found here: https://huggingface.co/transformers/v3.0.2/main_classes/pipelines.html\n",
    "\n",
    "We will use the pipeline module to load a fine-tuned model to perform senteiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a transformer model 'distilbert-base-uncased-finetuned-sst-2-english' that is fine-tuned for binary classification from the Hugging face repository:\n",
    "\n",
    "https://huggingface.co/models\n",
    "\n",
    "We need to load the model for the sequence classifcation and the tokenizer to convert the sentences into tokens according to the vocabulary of the model.\n",
    "\n",
    "Loading the model takes some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amina\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentimentenglish = pipeline(\"sentiment-analysis\", \n",
    "                            model=\"distilbert-base-uncased-finetuned-sst-2-english\", \n",
    "                            tokenizer=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now created an instantiation of a pipeline that can tokenize any sentence, obtain a sententence embedding from the transformer language model and perform the **sentiment-analysis** task. Let's try it out on an example sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pos_en = \"Nice hotel and the service is great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.999881386756897}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentenglish(sentence_pos_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_neg_en = \"The rooms are dirty and the wifi does not work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997870326042175}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentenglish(sentence_neg_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is easy and seems to work very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Dutch fine-tuned transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a fine-tuned Dutch model for Dutch sentiment analysis by creating another pipeline. Again loading this model takes some time. Also note that after loading, both moodels are loaded in memory. So if you have issues loading, you may want to start over and try again just with the Dutch pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\amina\\.cache\\huggingface\\hub\\models--wietsedv--bert-base-dutch-cased-finetuned-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentimentdutch = pipeline(\"sentiment-analysis\", \n",
    "                          model=\"wietsedv/bert-base-dutch-cased-finetuned-sentiment\", \n",
    "                          tokenizer=\"wietsedv/bert-base-dutch-cased-finetuned-sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test it on two similar Dutch sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pos_nl=\"Mooi hotel en de service is geweldig\"\n",
    "sentence_neg_nl=\"De kamers zijn smerig en de wifi doet het niet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'pos', 'score': 0.9999955892562866}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentdutch(sentence_pos_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'neg', 'score': 0.6675444841384888}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentimentdutch(sentence_neg_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work fine too although the score for negative in the second example is much lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting sentence representations using Simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Simpletransformers package is built on top of the transformer package. It simplifies the use of transformers even more and provides excellent documentation: https://simpletransformers.ai\n",
    "\n",
    "The site explains also how you can fine-tune models yourself or even how to build models from scratch, assuming you have the computing power and the data.\n",
    "\n",
    "Here we are going to use it to inspect the sentence representations a bit more. Unfortunately, we need to load the English model again as an instantiation of a RepresentationModel. So if you have memory issues, please stop the kernel and start again from here.\n",
    "\n",
    "Loading the model may gave a lot of warnings. You can ignore these. If you do not have a graphical card (GPU) and or cuda installed to use the GPU you need to set use_cuda to False, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of BertForTextRepresentation were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.language_representation import RepresentationModel\n",
    "        \n",
    "#sentences = [\"Example sentence 1\", \"Example sentence 2\"]\n",
    "model = RepresentationModel(\n",
    "        model_type=\"bert\",\n",
    "        model_name=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        use_cuda=False ## If you cannot use a GPU set this to false\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Representationmodel allows you to obtain a sentence encoding. We do that next for the positive English example which consists of 7 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice hotel and the service is great'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_pos_en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the simpletransformers API the input must be a list even when it is a single sentence. If you pass a string as input, it will turn it into a list of charcaters, each character as a separate sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.encode_sentences([sentence_pos_en], combine_strategy=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a numpy array with the shape (1, 9, 768) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 9, 768)\n"
     ]
    }
   ],
   "source": [
    "print(type(word_vectors))\n",
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first number indicates the number of sentences, which is **1** in our case. The next digit **9** indicates the number of tokens and the final digit is the number of dimension for each token according to the transformer model, which **768** in case of BERT models.\n",
    "\n",
    "We can ask for the full embedding representation for the first token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of dimensions for the mebdding of the first token: 768\n",
      "[-1.5398697  -1.0169228   1.0272177   1.2418951   1.2692713   0.87777656\n",
      " -0.7223468   1.3373877  -2.8084295   0.1980253  -0.8346974  -0.9953711\n",
      " -2.032871   -0.3409271   2.139137    1.9258752   0.85766643  2.5137255\n",
      "  0.67765296 -0.41850653 -1.4893218  -0.17095388  1.1302085   1.3729486\n",
      "  1.1725357  -0.21008687 -0.9759953  -1.3556827  -1.7344168  -0.6167605\n",
      " -1.2852031   0.61838925 -0.1619733   0.59804755 -0.4538428   0.6089023\n",
      " -1.1695803   0.08239149 -0.807556   -1.0054197  -0.48749292 -0.3784391\n",
      "  1.0347805   0.14620247  0.18716033 -1.4329507   0.49156392 -0.32463717\n",
      "  2.1962826   0.4746712   0.03687827  0.31791258  1.8357697   0.6402055\n",
      "  1.5319984   1.3642994   0.29719186  1.2101345   0.41975528 -0.82035357\n",
      " -0.07322589 -0.89692205  0.19344972  1.1281337   0.8287384  -0.7509966\n",
      " -0.58949244  0.27243912 -0.11436398  0.7038807  -0.6747096   0.8578785\n",
      "  0.79638165 -0.16919458  0.89852536 -1.1970075   1.8811995   0.64701116\n",
      " -0.5461626   1.6050516  -0.12011129  1.3084713  -0.6801845  -1.2858709\n",
      "  0.26449302  2.5851903  -0.8689102  -0.35376918  0.17039612  2.0030177\n",
      "  0.07214139  1.361794    0.22293611  0.09885632 -0.9996644   0.02561817\n",
      " -0.985852    0.9489172   0.88575697  0.23078574 -0.5025873  -0.51461285\n",
      " -0.4353462  -1.1378582  -1.9189404   1.5632657  -1.9115782  -0.52543193\n",
      "  1.3639618   0.66233224  0.5950419   0.7226076   0.6295081   0.11185338\n",
      "  0.76531464 -1.4788655   1.2588553   1.5915986   0.8875709   0.99326754\n",
      " -1.3820512  -0.6034894  -0.34764537 -0.9473052  -0.632515   -1.447905\n",
      "  1.2275422   0.6127212  -0.58165807 -0.00607347 -0.5980915  -1.0592223\n",
      "  0.12142542 -0.18925704  0.8472169  -1.5535725   1.0157889  -1.6728777\n",
      " -0.6599861   0.6538346  -0.33258107 -0.40486103 -0.28854963 -0.89686024\n",
      "  0.5164721  -0.45505846 -1.0153443  -0.59267044 -0.44765118 -0.05939513\n",
      " -0.9424404  -0.6640678  -0.16052543 -0.61095345 -0.42381307  0.81503385\n",
      " -0.38412446 -1.1691232  -0.60489714  0.47858232 -0.65828896 -0.9943283\n",
      "  0.36932775 -1.0061988   2.4173281  -0.54046476  0.8556017  -1.8616803\n",
      "  0.39150184  1.4095849   1.6946727   1.7839313  -0.3536966   1.5586153\n",
      "  0.30599627  0.64934766 -0.6101708   0.42326778  0.01138149  1.0369216\n",
      " -0.92603254 -0.9491915  -0.92588335 -0.6020151  -0.28742483  0.30968818\n",
      "  0.22939986 -0.8578935   0.533337    1.0474912   0.96687514 -0.8236959\n",
      " -0.78018606  1.7872393  -1.8669338  -0.22237004  1.3749171  -1.4351534\n",
      "  0.29037794 -0.5206291   0.2414597   1.4177362   1.3161744   0.14522249\n",
      "  0.76479584  0.46093065 -0.46492496 -0.17019501 -0.16632052  0.37126046\n",
      "  0.3221887   0.30068463 -0.43811285  0.861081    0.45355213 -0.70072657\n",
      " -0.05643949 -1.475952    0.09174267 -0.22800605 -0.9873888  -0.28061482\n",
      "  1.859735   -0.45135587  1.4838963  -0.8197598   0.20916562 -0.5210865\n",
      " -1.2989559   1.2826227  -0.70296836  1.6560704  -1.748159   -0.8755595\n",
      "  1.6641598  -0.06554985 -0.7056265  -0.39272872  0.55914474  0.01742295\n",
      " -0.03776481 -0.3727567   0.65241677  0.28736168  1.0401564  -0.46451607\n",
      "  0.6992774   0.6554096   0.99950284  0.5660113  -0.08142651  0.31487632\n",
      "  0.16180772 -0.04887388 -1.0038879  -0.05538673  1.7662995   0.05154451\n",
      "  0.9337174  -2.6738136   1.7320254   0.2817617  -0.40683228 -0.8072868\n",
      "  0.47888115 -0.82785624  0.57116634 -0.5536484   0.5629699   2.033136\n",
      " -1.2850521   1.582305   -0.47234562 -1.7651701  -1.8136699  -1.1073802\n",
      " -1.0086502   2.069925   -0.08286928 -0.44623014 -0.5611429   0.22029832\n",
      " -1.5846981   0.74376756 -0.24464168 -1.0427347   0.7154631  -0.7514464\n",
      " -0.20146601  1.3333113   1.6776567  -1.4966154  -0.8019074  -1.7316765\n",
      " -0.6977279  -1.0707184  -0.9305291   0.95883137  1.6663232   1.53347\n",
      "  1.7328072   1.5239679  -1.7598381  -1.5369716   0.7615393   1.2107707\n",
      " -0.06259145 -1.4190463  -0.4851886   0.92372197 -2.055102   -0.35694164\n",
      " -0.9155443  -0.1288143  -1.03205    -0.8677919  -1.4506937   1.0360225\n",
      "  1.1192758  -1.584477    0.94239867  0.63387936  1.4321473  -1.8128817\n",
      "  0.08485127 -0.62866855  0.05203886 -0.70987755 -0.04921425 -0.21095408\n",
      "  0.62427026 -0.15687796 -0.22866243 -0.5193838   0.7237943  -0.00556014\n",
      "  0.71763384  1.2965271   0.95245594  1.6304107  -0.10072654  0.88602287\n",
      " -0.22739477 -1.3238312   0.7437348  -1.138317    1.6516893  -1.7534035\n",
      "  0.7170219  -0.88491845 -0.38505906 -0.19668569  1.3487918  -1.6110185\n",
      "  0.06492422 -1.3356221   0.4975356  -0.38061386 -0.7957956  -0.1613438\n",
      "  0.687603   -0.64142406 -0.23484717 -0.23868017  1.7938266  -0.19125095\n",
      "  0.37229577  0.40968046 -1.7998888  -0.05863697 -0.50520265 -2.5329535\n",
      "  0.04669178 -0.4382405  -1.1421316   0.55705446  0.2853091  -0.13603833\n",
      "  0.55107003 -1.5729195  -0.19387376 -0.26007858  1.7778265  -0.42649606\n",
      " -0.05723708 -0.15801437  1.8060538  -0.646217   -0.92949224  0.4999622\n",
      "  1.5559679  -0.7912794  -1.8327423   0.66370404  0.07092942 -0.84103256\n",
      "  0.40130165  0.77261543 -1.5341794   0.44356224 -0.11167262  0.6676658\n",
      "  0.39343667 -0.34403422 -0.5007669   0.39828813  0.45716625 -2.8766067\n",
      " -0.7447943   0.05752906 -0.51339895  0.575771   -0.06036959  0.1581936\n",
      "  0.9069568  -0.44138238  0.73610663  1.2199363  -0.57637554 -0.28940156\n",
      " -0.5091493   2.5010118  -0.22351423 -1.4405696   1.6127605  -0.16395663\n",
      "  0.7995783   0.2845751   0.6847082   0.09213186 -0.45615718  0.06187654\n",
      " -2.4185662  -1.5858554   0.2670171   0.6440207  -0.6621551   0.2565099\n",
      " -0.3086069  -0.42431346 -0.42192122  0.6865187  -0.41853437  0.2977025\n",
      " -0.5145007   0.7218024  -0.04197802 -1.1750646   0.8644883  -1.0566002\n",
      "  1.6087224  -0.5260715  -0.7404369  -2.0346298   0.03782215  0.5183315\n",
      "  0.975429    0.12832591 -0.7021005  -1.1522415  -0.07147617 -0.81865215\n",
      "  0.10393028 -1.1983267  -1.1002976   0.86497945  1.0990956   1.418391\n",
      " -0.5444099   0.6655637   0.4507176   0.86129594  0.09277452 -0.13052472\n",
      " -1.2208507  -0.252961    0.28203717  0.6111891  -0.8783346   1.1499846\n",
      " -0.5570525  -1.4031391   1.1964597  -0.16510785  1.961902   -0.43336073\n",
      " -1.5794636  -0.58791655 -0.41577026 -1.3885099  -0.16215737  0.11790316\n",
      "  0.5569755  -0.48168305  0.5126454   0.539597   -0.8061806   0.7662253\n",
      "  0.56807256  0.45422378 -1.5538123  -1.1720616  -0.33923802  0.5960064\n",
      " -3.8377948  -0.17451255 -1.1144769   0.31353793  0.6387985  -1.4412379\n",
      "  0.62419665 -0.42094222  0.07027811 -1.346725    0.80431145  1.2581875\n",
      "  0.10501822 -1.393943    0.04866188  0.23129237  2.4794056   0.23540428\n",
      "  0.25019342 -0.44636065 -0.5019662   1.0438861   0.7418693   0.6076451\n",
      "  1.4412693  -0.36179003  0.7769325   0.05992737  0.69011754 -1.1587948\n",
      "  0.8554251   0.09006187 -0.08972314  0.168351    1.2376051   1.5133058\n",
      "  2.0354633   0.38385648  0.2742077  -1.4748706   0.13234545 -1.3831862\n",
      "  1.4721023  -0.01028408  0.5862483   0.2609193   0.04996501  1.0487759\n",
      "  0.80969584 -0.13267477 -0.35532317  0.3968564  -1.0092354   0.6103439\n",
      "  0.30593464  0.0809026  -0.7841235   1.5534495   0.81776893  0.1360127\n",
      " -1.396101    0.42655334 -1.768314    1.9217147  -0.16549122 -2.1697009\n",
      "  0.2715439   1.2104363  -0.05738614 -0.9861249  -0.1250119  -0.14271234\n",
      " -0.03891678  1.0349821  -0.8120536   0.02358904 -0.11506895  2.172579\n",
      " -0.14578748  0.5144702  -0.9440343  -0.9328259   0.5408666   0.32862553\n",
      " -1.7629979   0.5017908   1.3857845   1.1330129   0.7882875   0.5349862\n",
      "  0.54784864  1.8115231  -0.10587936 -1.9364433   1.6162608   1.3997504\n",
      " -0.8041345   1.0382478  -2.9614356  -0.4777836   0.46372688 -0.97028947\n",
      " -0.8508764   0.4716812   0.5471707  -1.6812063  -0.08928115  0.09554505\n",
      "  1.5514448  -0.93743044 -0.08674383 -0.36337784  0.01806854 -0.14990415\n",
      "  0.10056468 -0.5367972  -0.93535715 -0.13068414 -0.00494394 -1.0392611\n",
      "  1.4332509   2.9111981  -0.39894864  0.28565976  0.27328792 -2.6925085\n",
      " -0.7431283   0.5379449  -0.17405456 -0.36998162  0.5137912   1.984027\n",
      " -0.8759304  -0.4389724  -1.2845635   0.02004473  1.924536    0.24559675\n",
      " -1.0211898   0.8715943  -0.87094414 -0.1285242   0.11253816  1.3365172\n",
      " -0.5397815   0.3852558  -1.0358069   0.29910135 -1.1151565  -0.12348098\n",
      "  0.79111665 -1.9024929  -0.5762945  -2.304787    2.1300983  -0.8339944\n",
      "  0.11876505  0.97699153  1.1125829   0.6491594   0.60998046 -0.4095066\n",
      "  0.911388   -1.2070484  -0.6556687   0.20498869  1.2710991   0.8247964\n",
      "  0.64612067  1.898476    0.01449631 -0.59258443 -0.20337266  0.5761362\n",
      "  1.1767509  -2.7326949  -0.52239406  1.167747    0.6844173   0.40836933\n",
      " -0.75226116  1.3145715  -1.0931726   1.5549309   1.2376045  -0.24213678\n",
      " -0.15858768 -0.50186074 -0.22505897 -1.8413957   0.25438976  0.02762659\n",
      "  0.519604   -2.2436576   0.04919097  0.16174647 -0.5676104   0.14732654\n",
      "  1.481748    0.63965046  0.4299966  -0.5179016   1.0450921   0.15308824\n",
      " -0.6427592  -0.5442042   0.55588824 -0.5482486   0.29668954  0.17868198\n",
      "  0.7446904   0.3859646  -1.7587315   0.6336173   1.1085624   0.792971\n",
      " -0.15836534  0.4259437   1.9844673  -0.6404414   1.5973293  -0.8703433\n",
      " -0.47652334 -0.8106541  -0.03666982  1.0285076  -0.11919462 -1.0561639\n",
      " -0.394882   -1.3135017   0.14026181 -0.56623226 -0.9476149   0.98761225\n",
      " -1.6632195   0.5132383  -1.0837448   0.5569593  -0.72652394  0.93425757\n",
      "  0.79917276 -0.24913657  0.72530663  0.05637542 -1.4183463  -1.839971\n",
      "  1.2887292  -0.8770315  -0.08142965 -0.09243108  0.15535688 -1.2927741\n",
      " -1.3216386   1.1642585  -1.0003923  -0.28570905 -1.2303677  -0.18733682\n",
      "  0.33488992 -0.7490238   0.20085572  1.0732569  -0.28797206  0.38121176]\n"
     ]
    }
   ],
   "source": [
    "print('Nr of dimensions for the mebdding of the first token:', len(word_vectors[0][0]))\n",
    "print(word_vectors[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WAIT** Our sentence has 7 words so why do we get 9 tokens here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can  use the tokenizer of the model to get the token representation of the transformer and check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 100, 3309, 1998, 1996, 2326, 2003, 2307, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = model.tokenizer(sentence_pos_en)\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although our sentence has 7 words, we get 9 identifiers. We can use the **decode** function to convert them back to words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first token is the special token **CLS** which is an abstract sentence representation. Let's check another one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hotel'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(3309)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allright, this a word from our sentence. Let's decode them all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 [CLS]\n",
      "100 [UNK]\n",
      "3309 hotel\n",
      "1998 and\n",
      "1996 the\n",
      "2326 service\n",
      "2003 is\n",
      "2307 great\n",
      "102 [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenid_list = tokenized_sentence['input_ids']\n",
    "for token_id in tokenid_list:\n",
    "    print(token_id, model.tokenizer.decode(token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer model added the special tokens **CLS** and **SEP** but also represented our \"Nice\" with the **UNK** token. Any idea why? Check the name of the model we used....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the uncased model, which means that for training all inoput was downcased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
