{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-3 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "```\n",
    "- The words \"I\" and \"apples\" are neutral, while \"love\" is positive according to the VADER lexicon. At first glance it might seem odd that the positive score is higher than the neutral score, but this is reasonale since the word love has a very strong positive sentiment. The compound score of 0.6369 shows that VADER correctly identifies the sentence as positive overall.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "```\n",
    "- Given that the compound score of -0,5216 is close to -1, VADER considers the sentence to have a negative sentiment overall. While one might expect a higher positive score due to the use of the word \"love\", the use of \"don't\" negates the positive sentiment of the word love. This explains the positive score of 0. Overall, the results are reasonable.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "```\n",
    "- The sentence \"I love apples :-)\" has a compound score of 0.7579, which indicates an overall positive sentiment. The smiley face emoticon is recognised by VADER as having a positive effect, so the positive score (0.867) is slightly higher than in the sentence \"I love apples\" (which had a positive score of 0.808). Overall, the results are reasonable, as the smiley face further increases the sentiment.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "```\n",
    "\n",
    "- The negative score of 0.492 comes from the word \"ruins,\" which carries a negative sentiment. All of the other words are neutral and thus explain the neutral score of 0.508. Since there are no positive words in the sentence, the positive score is 0.0. The compound score of -0.4404 reflects the sentiment as slightly negative, but not strongly negative, due to the balance between the neutral and negative scores.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "```\n",
    "\n",
    "- VADER considers this sentence to have a slightly positive sentiment given that the compound score is about 0.5. The word \"certainly\" indicates a strong sentiment and the word \"ruins\", like in the sentence above, has a negative sentiment, but the use of the word \"not\" directly after it makes the sentiment positive. The positive and neutral socres are not that reasonable, because the use of the word certainly shows a stronger sentiment, so the positive value should have been a bit higher, making the neutral score lower.\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "```\n",
    "\n",
    "- The negative score of 0.286 likely comes from the word \"lies,\" which can have both a neutral meaning (i.e., lying down) and a negative one (i.e., telling lies). VADER could have interpreted it as negative, which is incorrect in this context. The rest of the sentence is neutral which explains the high neutral score (0.714).\n",
    "\n",
    "```\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```\n",
    "- The word \"like\" contributes to the positive score of 0.333 since it is associated with positive sentiment in the VADER lexicon. However, in this context the word \"like\" is used for comparison, so the positive score is incorrect. The neutral score of 0.667 is expected, as the sentence contains words like \"This\", \"house\" and \"is\" and does not have any lexicons with a strong sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream. If you have trouble accessing Twitter, try to find an existing dataset (on websites like kaggle or huggingface).\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source = https://www.kaggle.com/datasets/ahmedshahriarsakib/tweet-sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'positive', 'text_of_tweet': 'All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.', 'tweet_url': 'https://twitter.com/BarackObama/status/946775615893655552'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "        \n",
    "        The classification report shows the model's performance across three sentiment classes: negative, neutral, and positive. Precision indicates how accurate the model's predictions were for each class, while recall measures how well it identified all instances of each sentiment. The F1-score balances both precision and recall, with a higher score reflecting a better overall performance. The model performs well for negative sentiment, with high precision (0.79) and recall (0.94), but struggles with neutral sentiment, showing a lower recall (0.44) and F1-score (0.56). The weighted average F1-score of 0.72 suggests that while the model performs reasonably well overall, there is room for improvement, especially with neutral tweets.\n",
    "\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those.\n",
    "\n",
    "        POSITIVE TWEETS MISCLASSIFIED:\n",
    "        1. Tweet: Nothing like a fresh cup of coffee in the morning to start the day right! | Predicted as: negative\n",
    "        2. Tweet: What an incredible concert! One for the books! | Predicted as: neutral\n",
    "        3. Tweet: Just booked my first solo trip! Can't wait to explore new places. | Predicted as: neutral\n",
    "\n",
    "        NEGATIVE TWEETS MISCLASSIFIED:\n",
    "        1. Tweet: Can't believe how much the rent prices have gone up in this area. Unbelievable. | Predicted as: positive\n",
    "\n",
    "        NEUTRAL TWEETS MISCLASSIFIED:\n",
    "        1. Tweet: Just finished watching the new episode. It was okay, not great but not terrible either. | Predicted as: positive\n",
    "        2. Tweet: The new phone update has some interesting changes. | Predicted as: positive\n",
    "        3. Tweet: The movie I watched last night was just okay, not as good as the reviews said. | Predicted as: negative\n",
    "        4. Tweet: The news today was a mix of interesting stories. | Predicted as: positive\n",
    "        5. Tweet: Just read an interesting article. Not sure how I feel about it yet. | Predicted as: positive\n",
    "        6. Tweet: The book I’m reading is slow to start, but I’m hoping it picks up soon. | Predicted as: positive\n",
    "        7. Tweet: I’ve been thinking about starting a new fitness routine. | Predicted as: positive\n",
    "        8. Tweet: Not sure how to feel about the news today. | Predicted as: negative\n",
    "        9. Tweet: Weather is looking mild for the weekend. No surprises expected. | Predicted as: negative\n",
    "\n",
    "        The error analysis reveals that the VADER sentiment analysis model often misclassifies tweets beacause it relies on certain keywords that may not always capture the full context of the tweet. Positive tweets can be misclassified as negative or neutral because VADER sometimes misses subtle positive expressions or misinterprets neutral phrases as negative. In negative tweets, VADER might mistake words with dual meanings, like \"unbelievable,\" for positive sentiment. Neutral tweets are frequently misclassified as positive because the model focuses too much on positive words like \"interesting.\" These misclassifications show how difficult it can be to detect sentiment, when context and nuance are involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: This traffic is absolutely ridiculous. Been stuck for an hour and haven’t moved an inch.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: Just finished watching the new episode. It was okay, not great but not terrible either.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: Can’t believe I got the job! Excited for this new journey ahead! #blessed\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: Another rainy day... so much for making weekend plans. Ugh.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: Nothing like a fresh cup of coffee in the morning to start the day right!\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: Apple just announced their new product line-up. Thoughts?\n",
      "Predicted Sentiment: neutral\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: The customer service here is terrible. Been waiting for over an hour!\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: Had an amazing dinner at this new place downtown. Highly recommend!\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: The stock market showed mixed signals today.\n",
      "Predicted Sentiment: neutral\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: So proud of my little sister for graduating today! Huge milestone!\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: Got scammed online today. Feeling so frustrated!\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: The new phone update has some interesting changes.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: What an incredible concert! One for the books!\n",
      "Predicted Sentiment: neutral\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: Can’t believe my flight got delayed again. This is ridiculous.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: It's been a quiet day, not much happening.\n",
      "Predicted Sentiment: neutral\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: I just finished reading an amazing book! Highly recommend it to everyone.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: Can't believe how much the rent prices have gone up in this area. Unbelievable.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: Trying out a new recipe today. Let's see how it turns out.\n",
      "Predicted Sentiment: neutral\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: So grateful for all the love and support I've been receiving lately.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: I can't believe this happened. I'm so angry right now.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: The movie I watched last night was just okay, not as good as the reviews said.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: I just got back from vacation and it was the best trip ever!\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: The internet is down again. This is getting really frustrating.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: I can't decide what to eat for lunch today. Any suggestions?\n",
      "Predicted Sentiment: neutral\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: Just finished a run and feeling great! #healthylifestyle\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: My package still hasn't arrived yet. I’m starting to get annoyed.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: The news today was a mix of interesting stories.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: Can't wait for the weekend! Excited for some time to relax.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: I can’t believe how much I have to work this weekend. No break at all.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: Just read an interesting article. Not sure how I feel about it yet.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: Got to catch up with old friends today. Felt so good to reconnect.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: The weather forecast was wrong again. It’s pouring and I didn't bring an umbrella!\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: I’ve been hearing a lot of buzz around the new Netflix series.\n",
      "Predicted Sentiment: neutral\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: Feeling really proud of myself for achieving my goal this week!\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: I just realized I lost my wallet. Terrible start to the day.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: I've been meaning to start a new hobby. Any suggestions?\n",
      "Predicted Sentiment: neutral\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: Just booked my first solo trip! Can't wait to explore new places.\n",
      "Predicted Sentiment: neutral\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: I think I lost my keys. Not looking forward to finding them.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: It's been a productive day at work. Feels good to get things done.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: Excited to start a new project today! Let's do this.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: I’m really disappointed by the way this event turned out. Not what I expected.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: The book I’m reading is slow to start, but I’m hoping it picks up soon.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: Just made it to the gym for an awesome workout! Feeling strong.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: I’ve been feeling really stressed out lately. Need a break.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: I’ve been thinking about starting a new fitness routine.\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: Had the best day at the beach today. So relaxing!\n",
      "Predicted Sentiment: positive\n",
      "Actual Sentiment: positive\n",
      "--------------------------------------------------\n",
      "Tweet: My phone battery is draining too fast. It’s so annoying.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: negative\n",
      "--------------------------------------------------\n",
      "Tweet: Not sure how to feel about the news today.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "Tweet: Weather is looking mild for the weekend. No surprises expected.\n",
      "Predicted Sentiment: negative\n",
      "Actual Sentiment: neutral\n",
      "--------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.94      0.86        16\n",
      "     neutral       0.78      0.44      0.56        16\n",
      "    positive       0.68      0.83      0.75        18\n",
      "\n",
      "    accuracy                           0.74        50\n",
      "   macro avg       0.75      0.74      0.72        50\n",
      "weighted avg       0.75      0.74      0.72        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = vader_model.polarity_scores(the_tweet) # run vader\n",
    "    vader_label = vader_output_to_label(vader_output)# convert vader output to category\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "\n",
    "    # Print tweet and its classification\n",
    "    print(f\"Tweet: {the_tweet}\")\n",
    "    print(f\"Predicted Sentiment: {vader_label}\")\n",
    "    print(f\"Actual Sentiment: {tweet_info['sentiment_label']}\")\n",
    "    print('-' * 50)\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "print(classification_report(gold, all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,\n",
    "              verbose=0):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "        \n",
    "    input_to_vader = []\n",
    "\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_\n",
    "\n",
    "                if to_add == '-PRON-': \n",
    "                    to_add = token.text\n",
    "\n",
    "            if parts_of_speech_to_consider:\n",
    "                if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add) \n",
    "            else:\n",
    "                input_to_vader.append(to_add)\n",
    "\n",
    "    scores = vader_model.polarity_scores(' '.join(input_to_vader))\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        print()\n",
    "        print('INPUT SENTENCE', sent)\n",
    "        print('INPUT TO VADER', input_to_vader)\n",
    "        print('VADER OUTPUT', scores)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run VADER (as it is) on the set of airline tweets \n",
    "\n",
    "import os\n",
    "rootdir = '/Users/gebruiker/Documents/Naamloos/lab3/airlinetweets'\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        x = os.path.join(subdir, file)\n",
    "        if x.endswith('.txt'):\n",
    "            with open(x, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                print(content)\n",
    "                scores = vader_model.polarity_scores(content)\n",
    "                print(scores)\n",
    "                print() \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run VADER on the set of airline tweets after having lemmatized the text\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        x = os.path.join(subdir, file)\n",
    "        if x.endswith('.txt'):\n",
    "            with open(x, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                scores = vader_model.polarity_scores(content)\n",
    "                lemma = run_vader(content, lemmatize=True, verbose=1)\n",
    "                # print(scores)\n",
    "                # print()  # Separator between files\n",
    "                print(lemma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run VADER on the set of airline tweets with only adjectives\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        x = os.path.join(subdir, file)\n",
    "        if x.endswith('.txt'):\n",
    "            with open(x, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                print(content)\n",
    "\n",
    "                adjectives = run_vader(content, \n",
    "                            lemmatize=False, \n",
    "                            parts_of_speech_to_consider={'ADJ'},\n",
    "                            verbose=1)\n",
    "               \n",
    "                print(adjectives)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        x = os.path.join(subdir, file)\n",
    "        if x.endswith('.txt'):\n",
    "            with open(x, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                print(content)\n",
    "\n",
    "                adjectives = run_vader(content,lemmatize=True, parts_of_speech_to_consider={'ADJ'},verbose=1)\n",
    "               \n",
    "                print(adjectives)\n",
    "                print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run VADER on the set of airline tweets with only nouns\n",
    "\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        x = os.path.join(subdir, file)\n",
    "        if x.endswith('.txt'):\n",
    "            with open(x, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                print(content)\n",
    "\n",
    "                nouns = run_vader(content, \n",
    "                            lemmatize=False, \n",
    "                            parts_of_speech_to_consider={'NOUN'},\n",
    "                            verbose=1)\n",
    "               \n",
    "                print(nouns)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        x = os.path.join(subdir, file)\n",
    "        if x.endswith('.txt'):\n",
    "            with open(x, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                print(content)\n",
    "\n",
    "                nouns = run_vader(content, \n",
    "                            lemmatize=True, \n",
    "                            parts_of_speech_to_consider={'NOUN'},\n",
    "                            verbose=1)\n",
    "               \n",
    "                print(nouns)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run VADER on the set of airline tweets with only verbs\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        x = os.path.join(subdir, file)\n",
    "        if x.endswith('.txt'):\n",
    "            with open(x, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                print(content)\n",
    "\n",
    "                verbs = run_vader(content, \n",
    "                            lemmatize=False, \n",
    "                            parts_of_speech_to_consider={'VERB'},\n",
    "                            verbose=1)\n",
    "               \n",
    "                print(verbs)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        x = os.path.join(subdir, file)\n",
    "        if x.endswith('.txt'):\n",
    "            with open(x, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                print(content)\n",
    "\n",
    "                verbs = run_vader(content, \n",
    "                            lemmatize=True, \n",
    "                            parts_of_speech_to_consider={'VERB'},\n",
    "                            verbose=1)\n",
    "               \n",
    "                print(verbs)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which category performs best, is this the case for any setting?\n",
    "\n",
    "- \n",
    "\n",
    "Does the frequency threshold affect the scores? Why or why not according to you?\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86       364\n",
      "           1       0.84      0.71      0.77       282\n",
      "           2       0.84      0.86      0.85       305\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.82      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "import nltk\n",
    "import pathlib\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "path = str(airline_tweets_folder)\n",
    "airline_tweets_train = load_files(path)\n",
    "\n",
    "# TF-IDF, min_df=2\n",
    "#Feature extraction\n",
    "airline_vec = CountVectorizer(min_df=2, \n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "#Training with tf-idf model\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for testing\n",
    "    ) \n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       364\n",
      "           1       0.79      0.73      0.76       300\n",
      "           2       0.83      0.80      0.81       287\n",
      "\n",
      "    accuracy                           0.81       951\n",
      "   macro avg       0.81      0.80      0.81       951\n",
      "weighted avg       0.81      0.81      0.81       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF, min_df=5\n",
    "#Feature extraction\n",
    "airline_vec = CountVectorizer(min_df=5,\n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "#Training with tf-idf model\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for testing\n",
    "    ) \n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       362\n",
      "           1       0.79      0.72      0.75       297\n",
      "           2       0.83      0.81      0.82       292\n",
      "\n",
      "    accuracy                           0.81       951\n",
      "   macro avg       0.81      0.81      0.81       951\n",
      "weighted avg       0.81      0.81      0.81       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF, min_df=10\n",
    "#Feature extraction\n",
    "airline_vec = CountVectorizer(min_df=10, \n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "#Training with tf-idf model\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for testing\n",
    "    ) \n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       348\n",
      "           1       0.84      0.74      0.79       304\n",
      "           2       0.83      0.89      0.86       299\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.84      0.84       951\n",
      "weighted avg       0.84      0.84      0.84       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bag of words representation, min_df=2\n",
    "airline_vec = CountVectorizer(min_df=2, \n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "#Training with airline_counts model\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the airline_counts model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for testing\n",
    "    ) \n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.89      0.85       353\n",
      "           1       0.85      0.71      0.77       305\n",
      "           2       0.82      0.86      0.84       293\n",
      "\n",
      "    accuracy                           0.82       951\n",
      "   macro avg       0.82      0.82      0.82       951\n",
      "weighted avg       0.82      0.82      0.82       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bag of words representation, min_df=5\n",
    "airline_vec = CountVectorizer(min_df=5, \n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "#Training with airline_counts model\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the airline_counts model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for testing\n",
    "    ) \n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.92      0.88       346\n",
      "           1       0.81      0.75      0.78       326\n",
      "           2       0.85      0.81      0.83       279\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bag of words representation, min_df=10\n",
    "airline_vec = CountVectorizer(min_df=10, \n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "#Training with airline_counts model\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the airline_counts model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for testing\n",
    "    ) \n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=2, \n",
    "                             tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                             stop_words=stopwords.words('english')) # stopwords are removed\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "#Training with airline_counts model\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the airline_counts model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for testing\n",
    "    ) \n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important words in negative documents\n",
      "0 1492.0 @\n",
      "0 1371.0 united\n",
      "0 1219.0 .\n",
      "0 420.0 ``\n",
      "0 410.0 flight\n",
      "0 387.0 ?\n",
      "0 365.0 !\n",
      "0 316.0 #\n",
      "0 208.0 n't\n",
      "0 163.0 ''\n",
      "0 116.0 's\n",
      "0 110.0 :\n",
      "0 108.0 service\n",
      "0 95.0 virginamerica\n",
      "0 93.0 delayed\n",
      "0 92.0 get\n",
      "0 92.0 cancelled\n",
      "0 88.0 time\n",
      "0 88.0 bag\n",
      "0 83.0 plane\n",
      "0 82.0 customer\n",
      "0 79.0 -\n",
      "0 71.0 gate\n",
      "0 70.0 ;\n",
      "0 70.0 ...\n",
      "0 68.0 http\n",
      "0 68.0 'm\n",
      "0 66.0 hours\n",
      "0 65.0 still\n",
      "0 64.0 late\n",
      "0 63.0 hour\n",
      "0 63.0 airline\n",
      "0 63.0 &\n",
      "0 59.0 2\n",
      "0 57.0 would\n",
      "0 53.0 help\n",
      "0 53.0 amp\n",
      "0 52.0 ca\n",
      "0 51.0 one\n",
      "0 49.0 like\n",
      "0 49.0 flights\n",
      "0 47.0 us\n",
      "0 47.0 never\n",
      "0 46.0 waiting\n",
      "0 46.0 delay\n",
      "0 45.0 flightled\n",
      "0 44.0 3\n",
      "0 43.0 $\n",
      "0 42.0 due\n",
      "0 41.0 worst\n",
      "0 39.0 fly\n",
      "0 38.0 wait\n",
      "0 38.0 u\n",
      "0 38.0 (\n",
      "0 38.0 've\n",
      "0 37.0 people\n",
      "0 36.0 luggage\n",
      "0 36.0 day\n",
      "0 36.0 )\n",
      "0 35.0 really\n",
      "0 35.0 back\n",
      "0 34.0 trying\n",
      "0 34.0 another\n",
      "0 33.0 lost\n",
      "0 32.0 hold\n",
      "0 32.0 got\n",
      "0 32.0 bags\n",
      "0 31.0 ticket\n",
      "0 31.0 thanks\n",
      "0 31.0 seats\n",
      "0 31.0 last\n",
      "0 31.0 ever\n",
      "0 30.0 seat\n",
      "0 30.0 check\n",
      "0 29.0 today\n",
      "0 29.0 problems\n",
      "0 29.0 guys\n",
      "0 29.0 going\n",
      "0 29.0 crew\n",
      "0 29.0 baggage\n",
      "-----------------------------------------\n",
      "Important words in neutral documents\n",
      "1 1405.0 @\n",
      "1 524.0 ?\n",
      "1 494.0 .\n",
      "1 309.0 jetblue\n",
      "1 274.0 southwestair\n",
      "1 268.0 :\n",
      "1 266.0 #\n",
      "1 255.0 united\n",
      "1 249.0 ``\n",
      "1 233.0 flight\n",
      "1 190.0 americanair\n",
      "1 169.0 !\n",
      "1 167.0 http\n",
      "1 155.0 usairways\n",
      "1 134.0 's\n",
      "1 78.0 virginamerica\n",
      "1 77.0 get\n",
      "1 72.0 flights\n",
      "1 70.0 ''\n",
      "1 69.0 -\n",
      "1 68.0 please\n",
      "1 61.0 help\n",
      "1 57.0 )\n",
      "1 50.0 (\n",
      "1 48.0 need\n",
      "1 48.0 ;\n",
      "1 46.0 n't\n",
      "1 46.0 ...\n",
      "1 44.0 tomorrow\n",
      "1 42.0 dm\n",
      "1 41.0 &\n",
      "1 39.0 would\n",
      "1 38.0 know\n",
      "1 37.0 us\n",
      "1 37.0 flying\n",
      "1 37.0 fleet\n",
      "1 37.0 fleek\n",
      "1 37.0 'm\n",
      "1 36.0 ”\n",
      "1 35.0 “\n",
      "1 35.0 way\n",
      "1 34.0 hi\n",
      "1 34.0 cancelled\n",
      "1 32.0 thanks\n",
      "1 32.0 one\n",
      "1 32.0 change\n",
      "1 31.0 like\n",
      "1 31.0 amp\n",
      "1 29.0 could\n",
      "1 27.0 number\n",
      "1 27.0 new\n",
      "1 27.0 fly\n",
      "1 26.0 today\n",
      "1 26.0 destinationdragons\n",
      "1 24.0 see\n",
      "1 24.0 check\n",
      "1 23.0 go\n",
      "1 22.0 travel\n",
      "1 22.0 back\n",
      "1 22.0 airport\n",
      "1 21.0 time\n",
      "1 21.0 tickets\n",
      "1 21.0 ticket\n",
      "1 21.0 sent\n",
      "1 20.0 trying\n",
      "1 20.0 rt\n",
      "1 20.0 make\n",
      "1 20.0 follow\n",
      "1 19.0 start\n",
      "1 19.0 next\n",
      "1 19.0 guys\n",
      "1 19.0 dfw\n",
      "1 19.0 booked\n",
      "1 19.0 add\n",
      "1 19.0 2\n",
      "1 18.0 trip\n",
      "1 18.0 reservation\n",
      "1 18.0 morning\n",
      "1 18.0 chance\n",
      "1 17.0 weather\n",
      "-----------------------------------------\n",
      "Important words in positive documents\n",
      "2 1346.0 @\n",
      "2 1066.0 !\n",
      "2 775.0 .\n",
      "2 318.0 #\n",
      "2 299.0 jetblue\n",
      "2 296.0 southwestair\n",
      "2 290.0 thanks\n",
      "2 261.0 thank\n",
      "2 258.0 united\n",
      "2 244.0 ``\n",
      "2 181.0 flight\n",
      "2 175.0 americanair\n",
      "2 170.0 :\n",
      "2 135.0 usairways\n",
      "2 131.0 great\n",
      "2 93.0 service\n",
      "2 90.0 )\n",
      "2 86.0 virginamerica\n",
      "2 72.0 http\n",
      "2 72.0 customer\n",
      "2 68.0 love\n",
      "2 67.0 best\n",
      "2 65.0 guys\n",
      "2 64.0 much\n",
      "2 64.0 ;\n",
      "2 63.0 's\n",
      "2 58.0 awesome\n",
      "2 53.0 good\n",
      "2 48.0 airline\n",
      "2 48.0 -\n",
      "2 47.0 got\n",
      "2 46.0 &\n",
      "2 42.0 today\n",
      "2 41.0 n't\n",
      "2 41.0 amazing\n",
      "2 40.0 time\n",
      "2 40.0 get\n",
      "2 40.0 crew\n",
      "2 39.0 us\n",
      "2 38.0 amp\n",
      "2 37.0 help\n",
      "2 35.0 fly\n",
      "2 34.0 gate\n",
      "2 33.0 made\n",
      "2 33.0 home\n",
      "2 32.0 appreciate\n",
      "2 31.0 flying\n",
      "2 31.0 back\n",
      "2 30.0 response\n",
      "2 30.0 ''\n",
      "2 29.0 see\n",
      "2 28.0 work\n",
      "2 27.0 ?\n",
      "2 26.0 ever\n",
      "2 26.0 ...\n",
      "2 25.0 well\n",
      "2 25.0 u\n",
      "2 24.0 know\n",
      "2 24.0 flights\n",
      "2 24.0 (\n",
      "2 23.0 tonight\n",
      "2 23.0 new\n",
      "2 23.0 day\n",
      "2 22.0 staff\n",
      "2 22.0 like\n",
      "2 22.0 always\n",
      "2 22.0 're\n",
      "2 21.0 yes\n",
      "2 21.0 plane\n",
      "2 21.0 nice\n",
      "2 21.0 'm\n",
      "2 20.0 southwest\n",
      "2 20.0 getting\n",
      "2 19.0 quick\n",
      "2 19.0 make\n",
      "2 19.0 job\n",
      "2 19.0 happy\n",
      "2 19.0 finally\n",
      "2 19.0 agent\n",
      "2 19.0 'll\n"
     ]
    }
   ],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names_out()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
