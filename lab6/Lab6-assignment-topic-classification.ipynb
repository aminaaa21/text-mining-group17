{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab6-Assignment: Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the same training, development, and test partitions of the the 20 newsgroups text dataset as in Lab6.4-Topic-classification-BERT.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fine-tune and examine the performance of another transformer-based pretrained language models, e.g., RoBERTa, XLNet\n",
    "\n",
    "* Compare the performance of this model to the results achieved in Lab6.4-Topic-classification-BERT.ipynb and to a conventional machine learning approach (e.g., SVM, Naive Bayes) using bag-of-words or other engineered features of your choice. \n",
    "Describe the differences in performance in terms of Precision, Recall, and F1-score evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# load only a sub-selection of the categories (4 in our case)\n",
    "categories = ['alt.atheism', 'comp.graphics', 'sci.med', 'sci.space'] \n",
    "\n",
    "# remove the headers, footers and quotes (to avoid overfitting)\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=categories, random_state=42)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), categories=categories, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame({'text': newsgroups_train.data, 'labels': newsgroups_train.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame({'text': newsgroups_test.data, 'labels': newsgroups_test.target})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, dev = train_test_split(train, test_size=0.1, random_state=0, \n",
    "                               stratify=train[['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration # https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model \n",
    "model_args = ClassificationArgs()\n",
    "\n",
    "model_args.overwrite_output_dir=True # overwrite existing saved models in the same directory\n",
    "model_args.evaluate_during_training=True # to perform evaluation while training the model\n",
    "# (eval data should be passed to the training method)\n",
    "\n",
    "model_args.num_train_epochs=10 # number of epochs\n",
    "model_args.train_batch_size=32 # batch size\n",
    "model_args.learning_rate=4e-6 # learning rate\n",
    "model_args.max_seq_length=256 # maximum sequence length\n",
    "# Note! Increasing max_seq_len may provide better performance, but training time will increase. \n",
    "# For educational purposes, we set max_seq_len to 256.\n",
    "\n",
    "# Early stopping to combat overfitting: https://simpletransformers.ai/docs/tips-and-tricks/#using-early-stopping\n",
    "model_args.use_early_stopping=True\n",
    "model_args.early_stopping_delta=0.01 # \"The improvement over best_eval_loss necessary to count as a better checkpoint\"\n",
    "model_args.early_stopping_metric='eval_loss'\n",
    "model_args.early_stopping_metric_minimize=True\n",
    "model_args.early_stopping_patience=2\n",
    "model_args.evaluate_during_training_steps=32 # how often you want to run validation in terms of training steps (or batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationModel('roberta', 'roberta-base', num_labels=4, args=model_args, use_cuda=False) # CUDA is enabled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
