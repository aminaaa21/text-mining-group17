{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: VADER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset used: https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'sentiment_train.csv'\n",
    "train_data = pd.read_csv(train_path, on_bad_lines=\"warn\", encoding='latin1')\n",
    "train_texts = train_data['text'].astype(str).tolist()\n",
    "train_labels = np.array(train_data['sentiment']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'sentiment-topic-test.tsv' \n",
    "test_data = pd.read_csv(test_path, sep='\\t', on_bad_lines=\"warn\", encoding='latin1')\n",
    "test_texts = test_data['sentence'].astype(str).tolist()\n",
    "test_labels = np.array(test_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'neutral': 11118, 'positive': 8582, 'negative': 7781})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Frequency distribution of the training dataset\")\n",
    "Counter(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data proportion\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neutral': '40.46%', 'negative': '28.31%', 'positive': '31.23%'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_proportion = {}\n",
    "test_proportion = {}\n",
    "\n",
    "counter = dict(Counter(train_labels))\n",
    "\n",
    "sum_instances = 0\n",
    "\n",
    "for label in counter:\n",
    "    sum_instances += counter[label]\n",
    "\n",
    "for label in counter:\n",
    "    counter[label] = f\"{round((counter[label] / sum_instances)*100, ndigits=2)}%\"\n",
    "\n",
    "print(\"Training data proportion\")\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency distribution of the testing dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'positive': 6, 'neutral': 6, 'negative': 6})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Frequency distribution of the testing dataset\")\n",
    "Counter(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Bag of words representation, min_df=2\n",
    "vec = CountVectorizer(min_df=2, \n",
    "                             tokenizer=nltk.word_tokenize, \n",
    "                             stop_words=stopwords.words('english'))\n",
    "\n",
    "train_features = vec.fit_transform(train_texts)\n",
    "\n",
    "test_features = vec.transform(test_texts)\n",
    "\n",
    "model = MultinomialNB()\n",
    "clf = model.fit(train_features, train_labels)\n",
    "y_pred = clf.predict(test_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.33      0.36         6\n",
      "     neutral       0.45      0.83      0.59         6\n",
      "    positive       1.00      0.33      0.50         6\n",
      "\n",
      "    accuracy                           0.50        18\n",
      "   macro avg       0.62      0.50      0.48        18\n",
      "weighted avg       0.62      0.50      0.48        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The atmosphere at the stadium tonight was electric.\n",
      "Predicted Label: negative\n",
      "Actual Label: positive\n",
      "\n",
      "Text: The game was so intense I forgot to breathe at times. What a win!\n",
      "Predicted Label: negative\n",
      "Actual Label: positive\n",
      "\n",
      "Text: It had me hooked from the first chapter.\n",
      "Predicted Label: neutral\n",
      "Actual Label: positive\n",
      "\n",
      "Text: Itâs more of a slow burn than a page-turner, but itâs well-written, I guess.\n",
      "Predicted Label: negative\n",
      "Actual Label: neutral\n",
      "\n",
      "Text: Itâs split into two timelines, which keeps it interesting but also a bit confusing at times.\n",
      "Predicted Label: neutral\n",
      "Actual Label: neutral\n",
      "\n",
      "Text: I could watch this film a hundred times and still find something new to love about it.\n",
      "Predicted Label: neutral\n",
      "Actual Label: positive\n",
      "\n",
      "Text: Best thriller Iâve seen in ages. Had me on the edge of my seat the entire time.\n",
      "Predicted Label: positive\n",
      "Actual Label: positive\n",
      "\n",
      "Text: How do you concede three goals in ten minutes? The whole defence needs replacing.\n",
      "Predicted Label: neutral\n",
      "Actual Label: negative\n",
      "\n",
      "Text: They rotated their squad for the cup game, which wasnât surprising given the schedule.\n",
      "Predicted Label: neutral\n",
      "Actual Label: neutral\n",
      "\n",
      "Text: The trailer gave away most of the plot, but there were still a few surprises.\n",
      "Predicted Label: neutral\n",
      "Actual Label: neutral\n",
      "\n",
      "Text: The protagonist was so whiny I wanted to throw the book across the room.\n",
      "Predicted Label: neutral\n",
      "Actual Label: negative\n",
      "\n",
      "Text: The authorâs writing style is so unique â poetic, but not over the top.\n",
      "Predicted Label: positive\n",
      "Actual Label: positive\n",
      "\n",
      "Text: I don't get how was it supposed to work without any chemistry between the leads.\n",
      "Predicted Label: negative\n",
      "Actual Label: negative\n",
      "\n",
      "Text: It's still 0-0 so far, so way too early to tell - both teams trying their hardest, but maybe it won't be enough?\n",
      "Predicted Label: neutral\n",
      "Actual Label: neutral\n",
      "\n",
      "Text: I don't get the appeal at all, it's just a couple guys kicking a ball around at the end of the day.\n",
      "Predicted Label: neutral\n",
      "Actual Label: negative\n",
      "\n",
      "Text: Did you hear the screenplay for it was originally written on a napkin?\n",
      "Predicted Label: neutral\n",
      "Actual Label: neutral\n",
      "\n",
      "Text: It's really incredibly impressive to mess up such a tested blockbuster formula.\n",
      "Predicted Label: negative\n",
      "Actual Label: negative\n",
      "\n",
      "Text: The only way it's helped me is by keeping my table from being wobbly.\n",
      "Predicted Label: neutral\n",
      "Actual Label: negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text, pred, actual in zip(test_texts, y_pred, test_labels):\n",
    "    print(f\"Text: {text}\\nPredicted Label: {pred}\\nActual Label: {actual}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model struggles with negative sentiment. This can be seen in the precision (0.40) and recall (0.33) values for the negative label. On the other hand neutral sentiment has a fairly high recall (0.83), but a lower precision (0.45). This could be due to the training data having a lot of neutral instances (around 40%). For positive sentiment the model has really high precision (1), so all predicted positives are also actual positives but the low recall (0.33) indicates that other instances positive instances are not being classified as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- choices we have to justify in the poster:\n",
    "    + why naive bayes -> literature\n",
    "\n",
    "    + min_df value -> compare results when df=5 or df=10, performs worse bc data is in short tweet-like sentences, so a lot of important words don't have that high frequency but should still be included when determining sentiment. Setting min_df too high risks losing these key words.\n",
    "\n",
    "    + BoW representation -> back up with literature: focus on how it works and that it works well with NB\n",
    "\n",
    "    + removing stopwords -> compare results when not removing stopwords -> removing stopwords allows model to focus more on words that actually help determine sentiment and ignores common function words (e.g., the, is etc.)\n",
    "\n",
    "    + why nltk tokenisation vs letting CountVectorizer handle tokenisation -> literature: explain how nltk tokenises (handles punctuation, contractions, special cases) and how CountVectoriser handles tokenisation (simple whitespaces or punctuation-based splitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
