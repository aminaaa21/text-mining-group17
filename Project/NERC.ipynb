{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NERC model: SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction import DictVectorizer \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'ner_dataset.csv'\n",
    "\n",
    "dataset = pd.read_csv(path, encoding='latin1')\n",
    "dataset = dataset.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"NER-test.tsv\"\n",
    "\n",
    "# Read the file and process each line\n",
    "fixed_lines = []\n",
    "\n",
    "with open(path, \"r\", encoding=\"latin1\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        parts = line.strip().split(\"\\t\")  # Split by tab\n",
    "        parts = [p.strip() for p in parts]  # Remove leading/trailing spaces\n",
    "        if len(parts) != 4:  # Check for incorrect column count\n",
    "            # If there are more than 4 columns, merge extra parts into the last column\n",
    "            if len(parts) > 4:\n",
    "                parts = parts[:3] + [\" \".join(parts[3:])]  # Merge extras into the last column\n",
    "        fixed_lines.append(\"\\t\".join(parts))  # Keep the fixed line\n",
    "\n",
    "# Rewrite the original file with the fixed data\n",
    "with open(path, \"w\", encoding=\"latin1\") as file:\n",
    "    file.write(\"\\n\".join(fixed_lines))\n",
    "\n",
    "# Load the fixed data into pandas\n",
    "test_dataset = pd.read_csv(path, sep=\"\\t\", encoding=\"latin1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = []\n",
    "\n",
    "pos_list = dataset[\"POS\"].values\n",
    "token_list = dataset[\"Word\"].values\n",
    "\n",
    "for token in token_list:\n",
    "    a_dict = {\n",
    "        'words':token\n",
    "        }\n",
    "    train_text.append(a_dict)\n",
    "    \n",
    "\n",
    "train_labels = dataset[\"Tag\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = []\n",
    "\n",
    "token_list = test_dataset[\"token\"].values\n",
    "\n",
    "for token in token_list:\n",
    "    a_dict = {\n",
    "        'words':token\n",
    "        }\n",
    "    test_text.append(a_dict)\n",
    "\n",
    "test_labels = test_dataset[\"BIO_NER_tag\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "all_features = train_text + test_text\n",
    "the_array = vec.fit_transform(all_features)\n",
    "\n",
    "len_training_features = len(train_text)\n",
    "kaggle_training_features = the_array[:len_training_features]\n",
    "kaggle_test_features = the_array[len_training_features:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'O': 186,\n",
       "         'B-PERSON': 11,\n",
       "         'I-WORK_OF_ART': 10,\n",
       "         'B-WORK_OF_ART': 9,\n",
       "         'I-PERSON': 8,\n",
       "         'B-LOC': 7,\n",
       "         'B-ORG': 3,\n",
       "         'I-ORG': 2,\n",
       "         'I-LOC': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "Counter(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[{'words': 'I'} {'words': 'would'} {'words': \"n't\"} {'words': 'have'}\n {'words': 'gone'} {'words': 'to'} {'words': 'that'}\n {'words': 'Manchester'} {'words': 'United'} {'words': 'match'}\n {'words': 'if'} {'words': 'my'} {'words': 'cousin'} {'words': 'had'}\n {'words': \"n't\"} {'words': 'driven'} {'words': 'me'} {'words': 'there'}\n {'words': '.'} {'words': 'Elena'} {'words': 'and'} {'words': 'her'}\n {'words': 'family'} {'words': 'moved'} {'words': 'to'}\n {'words': 'Barcelona'} {'words': 'last'} {'words': 'summer'}\n {'words': 'to'} {'words': 'enjoy'} {'words': 'a'} {'words': 'warm'}\n {'words': 'weather'} {'words': '.'} {'words': 'The'} {'words': 'concert'}\n {'words': 'by'} {'words': 'Coldplay'} {'words': 'at'}\n {'words': 'Wembley'} {'words': 'Stadium'} {'words': 'was'}\n {'words': 'absolutely'} {'words': 'electrifying'} {'words': '!'}\n {'words': 'Lionel'} {'words': 'Messi'} {'words': 'made'} {'words': 'his'}\n {'words': 'debut'} {'words': 'for'} {'words': 'Inter'} {'words': 'Miami'}\n {'words': 'and'} {'words': 'proved'} {'words': 'why'} {'words': 'he'}\n {'words': 'is'} {'words': 'the'} {'words': 'GOAT'} {'words': '.'}\n {'words': 'I'} {'words': \"can't\"} {'words': 'believe'} {'words': 'how'}\n {'words': 'much'} {'words': 'I'} {'words': 'cried'} {'words': 'watching'}\n {'words': 'Titanic'} {'words': 'for'} {'words': 'the'} {'words': '10th'}\n {'words': 'time'} {'words': '.'} {'words': 'My'} {'words': 'friend'}\n {'words': 'Sarah'} {'words': 'just'} {'words': 'finished'}\n {'words': 'reading'} {'words': 'The'} {'words': 'Catcher'}\n {'words': 'in'} {'words': 'the'} {'words': 'Rye'} {'words': 'and'}\n {'words': \"couldn't\"} {'words': 'stop'} {'words': 'talking'}\n {'words': 'about'} {'words': 'Holden'} {'words': 'Caulfield'}\n {'words': '.'} {'words': 'Maria'} {'words': 'Sharapova'} {'words': \"'s\"}\n {'words': 'final'} {'words': 'game'} {'words': 'against'}\n {'words': 'Serena'} {'words': 'Williams'} {'words': 'is'}\n {'words': 'still'} {'words': 'one'} {'words': 'of'} {'words': 'my'}\n {'words': 'favorite'} {'words': 'matches'} {'words': '.'} {'words': 'I'}\n {'words': 'spotted'} {'words': 'a'} {'words': 'Banksy'}\n {'words': 'mural'} {'words': 'whilst'} {'words': 'walking'}\n {'words': 'through'} {'words': 'the'} {'words': 'streets'}\n {'words': 'of'} {'words': 'Bristol'} {'words': '!'} {'words': 'The'}\n {'words': 'Harry'} {'words': 'Potter'} {'words': 'series'}\n {'words': 'will'} {'words': 'always'} {'words': 'be'} {'words': 'my'}\n {'words': 'go-to'} {'words': 'comfort'} {'words': 'read'} {'words': '.'}\n {'words': 'During'} {'words': 'my'} {'words': 'trip'} {'words': 'to'}\n {'words': 'Berlin'} {'words': ','} {'words': 'I'} {'words': 'visited'}\n {'words': 'the'} {'words': 'Brandenburg'} {'words': 'Gate'}\n {'words': 'and'} {'words': 'learned'} {'words': 'so'} {'words': 'much'}\n {'words': 'about'} {'words': 'German'} {'words': 'history'}\n {'words': '.'} {'words': 'I'} {'words': 'just'} {'words': 'finished'}\n {'words': 'watching'} {'words': 'Stranger'} {'words': 'Things'}\n {'words': ','} {'words': 'and'} {'words': 'Eleven'} {'words': 'is'}\n {'words': 'hands'} {'words': 'down'} {'words': 'my'}\n {'words': 'favorite'} {'words': 'character'} {'words': '.'}\n {'words': 'I'} {'words': 'started'} {'words': 'reading'}\n {'words': '1984'} {'words': ','} {'words': 'and'} {'words': 'George'}\n {'words': 'Orwell'} {'words': \"'s\"} {'words': 'vision'} {'words': 'of'}\n {'words': 'a'} {'words': 'dystopian'} {'words': 'future'} {'words': 'is'}\n {'words': 'chilling'} {'words': '.'} {'words': 'Barbie'} {'words': 'and'}\n {'words': 'Oppenheimer'} {'words': 'releasing'} {'words': 'on'}\n {'words': 'the'} {'words': 'same'} {'words': 'day'} {'words': 'was'}\n {'words': 'the'} {'words': 'cinematic'} {'words': 'event'}\n {'words': 'of'} {'words': 'the'} {'words': 'decade'} {'words': '.'}\n {'words': 'I'} {'words': 'stumbled'} {'words': 'across'} {'words': 'a'}\n {'words': 'rare'} {'words': 'edition'} {'words': 'of'} {'words': 'To'}\n {'words': 'Kill'} {'words': 'a'} {'words': 'Mockingbird'} {'words': 'at'}\n {'words': 'a'} {'words': 'small'} {'words': 'bookstore'} {'words': 'in'}\n {'words': 'Dublin'} {'words': '.'} {'words': 'I'} {'words': 'just'}\n {'words': 'binged'} {'words': 'The'} {'words': 'Crown'} {'words': ','}\n {'words': 'and'} {'words': 'Claire'} {'words': 'Foy'} {'words': 'as'}\n {'words': 'Queen'} {'words': 'Elizabeth'} {'words': 'II'}\n {'words': 'was'} {'words': 'phenomenal'} {'words': '.'}].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m lin_clf \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mLinearSVC()\n\u001b[0;32m      2\u001b[0m lin_clf\u001b[38;5;241m.\u001b[39mfit(kaggle_training_features,train_labels)\n\u001b[1;32m----> 5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mlin_clf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(test_labels, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:374\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;124;03m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    373\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 374\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    376\u001b[0m     indices \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, indexing_dtype(xp))\n",
      "File \u001b[1;32mc:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\linear_model\\_base.py:351\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    348\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    349\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 351\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    354\u001b[0m     xp\u001b[38;5;241m.\u001b[39mreshape(scores, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (scores\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m scores\n\u001b[0;32m    357\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\utils\\validation.py:1093\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1087\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1088\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1091\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1092\u001b[0m             )\n\u001b[1;32m-> 1093\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1098\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1099\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[{'words': 'I'} {'words': 'would'} {'words': \"n't\"} {'words': 'have'}\n {'words': 'gone'} {'words': 'to'} {'words': 'that'}\n {'words': 'Manchester'} {'words': 'United'} {'words': 'match'}\n {'words': 'if'} {'words': 'my'} {'words': 'cousin'} {'words': 'had'}\n {'words': \"n't\"} {'words': 'driven'} {'words': 'me'} {'words': 'there'}\n {'words': '.'} {'words': 'Elena'} {'words': 'and'} {'words': 'her'}\n {'words': 'family'} {'words': 'moved'} {'words': 'to'}\n {'words': 'Barcelona'} {'words': 'last'} {'words': 'summer'}\n {'words': 'to'} {'words': 'enjoy'} {'words': 'a'} {'words': 'warm'}\n {'words': 'weather'} {'words': '.'} {'words': 'The'} {'words': 'concert'}\n {'words': 'by'} {'words': 'Coldplay'} {'words': 'at'}\n {'words': 'Wembley'} {'words': 'Stadium'} {'words': 'was'}\n {'words': 'absolutely'} {'words': 'electrifying'} {'words': '!'}\n {'words': 'Lionel'} {'words': 'Messi'} {'words': 'made'} {'words': 'his'}\n {'words': 'debut'} {'words': 'for'} {'words': 'Inter'} {'words': 'Miami'}\n {'words': 'and'} {'words': 'proved'} {'words': 'why'} {'words': 'he'}\n {'words': 'is'} {'words': 'the'} {'words': 'GOAT'} {'words': '.'}\n {'words': 'I'} {'words': \"can't\"} {'words': 'believe'} {'words': 'how'}\n {'words': 'much'} {'words': 'I'} {'words': 'cried'} {'words': 'watching'}\n {'words': 'Titanic'} {'words': 'for'} {'words': 'the'} {'words': '10th'}\n {'words': 'time'} {'words': '.'} {'words': 'My'} {'words': 'friend'}\n {'words': 'Sarah'} {'words': 'just'} {'words': 'finished'}\n {'words': 'reading'} {'words': 'The'} {'words': 'Catcher'}\n {'words': 'in'} {'words': 'the'} {'words': 'Rye'} {'words': 'and'}\n {'words': \"couldn't\"} {'words': 'stop'} {'words': 'talking'}\n {'words': 'about'} {'words': 'Holden'} {'words': 'Caulfield'}\n {'words': '.'} {'words': 'Maria'} {'words': 'Sharapova'} {'words': \"'s\"}\n {'words': 'final'} {'words': 'game'} {'words': 'against'}\n {'words': 'Serena'} {'words': 'Williams'} {'words': 'is'}\n {'words': 'still'} {'words': 'one'} {'words': 'of'} {'words': 'my'}\n {'words': 'favorite'} {'words': 'matches'} {'words': '.'} {'words': 'I'}\n {'words': 'spotted'} {'words': 'a'} {'words': 'Banksy'}\n {'words': 'mural'} {'words': 'whilst'} {'words': 'walking'}\n {'words': 'through'} {'words': 'the'} {'words': 'streets'}\n {'words': 'of'} {'words': 'Bristol'} {'words': '!'} {'words': 'The'}\n {'words': 'Harry'} {'words': 'Potter'} {'words': 'series'}\n {'words': 'will'} {'words': 'always'} {'words': 'be'} {'words': 'my'}\n {'words': 'go-to'} {'words': 'comfort'} {'words': 'read'} {'words': '.'}\n {'words': 'During'} {'words': 'my'} {'words': 'trip'} {'words': 'to'}\n {'words': 'Berlin'} {'words': ','} {'words': 'I'} {'words': 'visited'}\n {'words': 'the'} {'words': 'Brandenburg'} {'words': 'Gate'}\n {'words': 'and'} {'words': 'learned'} {'words': 'so'} {'words': 'much'}\n {'words': 'about'} {'words': 'German'} {'words': 'history'}\n {'words': '.'} {'words': 'I'} {'words': 'just'} {'words': 'finished'}\n {'words': 'watching'} {'words': 'Stranger'} {'words': 'Things'}\n {'words': ','} {'words': 'and'} {'words': 'Eleven'} {'words': 'is'}\n {'words': 'hands'} {'words': 'down'} {'words': 'my'}\n {'words': 'favorite'} {'words': 'character'} {'words': '.'}\n {'words': 'I'} {'words': 'started'} {'words': 'reading'}\n {'words': '1984'} {'words': ','} {'words': 'and'} {'words': 'George'}\n {'words': 'Orwell'} {'words': \"'s\"} {'words': 'vision'} {'words': 'of'}\n {'words': 'a'} {'words': 'dystopian'} {'words': 'future'} {'words': 'is'}\n {'words': 'chilling'} {'words': '.'} {'words': 'Barbie'} {'words': 'and'}\n {'words': 'Oppenheimer'} {'words': 'releasing'} {'words': 'on'}\n {'words': 'the'} {'words': 'same'} {'words': 'day'} {'words': 'was'}\n {'words': 'the'} {'words': 'cinematic'} {'words': 'event'}\n {'words': 'of'} {'words': 'the'} {'words': 'decade'} {'words': '.'}\n {'words': 'I'} {'words': 'stumbled'} {'words': 'across'} {'words': 'a'}\n {'words': 'rare'} {'words': 'edition'} {'words': 'of'} {'words': 'To'}\n {'words': 'Kill'} {'words': 'a'} {'words': 'Mockingbird'} {'words': 'at'}\n {'words': 'a'} {'words': 'small'} {'words': 'bookstore'} {'words': 'in'}\n {'words': 'Dublin'} {'words': '.'} {'words': 'I'} {'words': 'just'}\n {'words': 'binged'} {'words': 'The'} {'words': 'Crown'} {'words': ','}\n {'words': 'and'} {'words': 'Claire'} {'words': 'Foy'} {'words': 'as'}\n {'words': 'Queen'} {'words': 'Elizabeth'} {'words': 'II'}\n {'words': 'was'} {'words': 'phenomenal'} {'words': '.'}].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(kaggle_training_features,train_labels)\n",
    "kaggle_test_features = vec.transform(test_text)\n",
    "\n",
    "y_pred = lin_clf.predict(kaggle_test_features)\n",
    "\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        B-LOC       0.00      0.00      0.00         7\n",
      "        B-ORG       0.00      0.00      0.00         3\n",
      "     B-PERSON       0.00      0.00      0.00        11\n",
      "B-WORK_OF_ART       0.00      0.00      0.00         9\n",
      "        B-eve       0.00      0.00      0.00         0\n",
      "        B-geo       0.00      0.00      0.00         0\n",
      "        B-gpe       0.00      0.00      0.00         0\n",
      "        B-org       0.00      0.00      0.00         0\n",
      "        B-per       0.00      0.00      0.00         0\n",
      "        B-tim       0.00      0.00      0.00         0\n",
      "        I-LOC       0.00      0.00      0.00         1\n",
      "        I-ORG       0.00      0.00      0.00         2\n",
      "     I-PERSON       0.00      0.00      0.00         8\n",
      "I-WORK_OF_ART       0.00      0.00      0.00        10\n",
      "        I-org       0.00      0.00      0.00         0\n",
      "        I-per       0.00      0.00      0.00         0\n",
      "            O       0.88      0.98      0.93       186\n",
      "\n",
      "     accuracy                           0.77       237\n",
      "    macro avg       0.05      0.06      0.05       237\n",
      " weighted avg       0.69      0.77      0.73       237\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\amina\\anaconda3\\envs\\text-mining\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "kaggle_test_features = vec.transform(test_text)\n",
    "\n",
    "y_pred = lin_clf.predict(kaggle_test_features)\n",
    "\n",
    "print(classification_report(test_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
